---
title: "Gradient Boosting Machine.Rmd"
author: "Logan Schneider"
date: "September 30, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown  
```{r FYI, echo=FALSE}
#print("Analyses perfomed using:")
#R.Version()$version.string
print("gbm, glmnet, mlbench, foreign, rms, pscl, caret, pROC")
library(gbm)
library(glmnet)
library(mlbench)
library(foreign)
library(rms)
library(pscl)
library(caret)
library(pROC)
library(car)
library(agricolae)
print("Please use the following citation information")
citation()

sessionInfo()
```

```{r setup, include=FALSE}
#library(gbm)
#library(glmnet)
#library(mlbench)
#library(foreign)
setwd("C:/Users/xj901087/Dropbox/Stanford_Summer_Student")
MSLT <- read.csv("MASTER4R.csv")
summary(MSLT)

head(MSLT)
levels(MSLT$diagnosis)
MSLT$nom <- factor(MSLT$diagnosis)
levels(MSLT$nom)
MSLT$diagnosis2 <- relevel(MSLT$nom, ref = "4")
#remove hypocretin measurement as it shouldn't be included in the models
MSLTsub <- MSLT[,-2]
#now need to remove incomplete cases from the normal controls
nc<-MSLTsub[which(MSLTsub$diagnosis==4),]
head(nc)
tail(nc)
summary(nc)
str(nc)
ncomplete <- nc[complete.cases(nc),]
head(ncomplete)
tail(ncomplete)
summary(ncomplete)
str(ncomplete)
MSLTnew<-rbind.data.frame(MSLTsub[which(MSLTsub$diagnosis != 4),],ncomplete)
#defining categories by name and re-leveling to NC as the baseline
MSLTcat <- MSLTnew
MSLTcat$Dx <- "NA"
MSLTcat[which(MSLTcat$diagnosis == 1),"Dx"] <- "NT1"
MSLTcat[which(MSLTcat$diagnosis == 2),"Dx"] <- "NT2"
MSLTcat[which(MSLTcat$diagnosis == 3),"Dx"] <- "IH"
MSLTcat[which(MSLTcat$diagnosis == 4),"Dx"] <- "NC"
MSLTcat$nom <- factor(MSLTcat$Dx)
MSLTcat$diagnosis2 <- relevel(MSLTcat$nom, ref = "NC")
#subsetting for model creation
MSLTglmnet <- MSLTcat[,c(28,2,4,7,8,10,12,14,16,17)] 
#seems ridiculous to include gender, age, ethnicity in these types of models because sampling was biased: MSLTglmnet <- MSLTcat[,c(28,2,4,7,8,10,12,14,16,17,18,19,20,21,22,23,24,25,26)]
#most models require complete cases (i.e. non-sparse/no-NA data)
MSLTcc <- MSLTglmnet[complete.cases(MSLTglmnet),]
summary(MSLTcc)
```

```{r define training (5/8) and test/hold-out sets (3/8), echo=FALSE}
#set.seed(123)
#train=sample(1:nrow(MSLTcc),round(0.625*nrow(MSLTcc)))
#boostTrain = MSLTcc[train,]
#boostTrain
#boostTest=MSLTcc[-train,]

###this is better done with createDataPartition() because it ensures each category has an appropriate proportion between training and test
#library(caret)
set.seed(123)
trainpart <- createDataPartition(MSLTcc$diagnosis2,
                                 times=1,
                                 p=0.625,
                                 list=F)
boostTrain <- MSLTcc[trainpart,]
summary(boostTrain)
boostTest <- MSLTcc[-trainpart,]
summary(boostTest)
```

```{r whole model formulation}
###Why a shrinkage of 0.01? Why an interaction.depth of 4?
#A shrinkage of 0.01 was used because small shrinkage normally gives better results, but you end up having to do far more iterations. 
#Interaction depth means the number of splits that the generalized boosted model will perform on the tree, and because the model is generating 10,000 trees, more shallow are easier to interpret. There will be n*3+1 final nodes (it was found that decision trees and rpart ended with 8-9 terminal nodes, so 3-4 interaction depth is fine)
###Why a bag fraction of 0.5?
#The bag.fraction helps introduce more randomness to the data of which you choose to subset
set.seed(504)
boost.MSLT=gbm(diagnosis2~.,
               data=boostTrain,
               distribution="multinomial",
               n.trees=10000,
               shrinkage=0.01,
               interaction.depth=4,
               cv.folds=5,
               bag.fraction=0.5,
               verbose=TRUE)

boost.MSLT
png("Boost_variable_importance.png",
    width = 4,
    height = 4,
    units = 'in',
    bg = "transparent",
    res = 300)
summary(boost.MSLT)[1]
dev.off()
write.csv(summary(boost.MSLT,plotit=FALSE),"Boost_variable_importance.csv")
```

```{r partial plots for top 50% of variables}
#explore variable partial plots for variables in to 50% of relativel influence
explorevars <- summary(boost.MSLT,plotit=F)[which(summary(boost.MSLT,plotit=F)$rel.inf>=median(summary(boost.MSLT,plotit=F)$rel.inf)),]
rel.inf.vars <- row.names(explorevars)

#legends based on: https://stackoverflow.com/questions/20328452/legend-for-random-forest-plot-in-r
for(i in rel.inf.vars) {
    png(paste(i,"gbm partial plot.png"),
        width = 4,
        height = 4,
        units = 'in',
        bg = "white",
        res = 300)
    plot(boost.MSLT,i=i)
    legend("right",
           boost.MSLT$classes,
           col=1:boost.MSLT$num.classes,
           cex=0.8,
           fill=1:boost.MSLT$num.classes,
           bg="transparent")
    dev.off()
}
```

```{r choosing optimal iteration}
#****check performance using an out-of-bag estimator
#****OOB underestimates the optimal number of iterations

gbm.perf(boost.MSLT, plot.it=TRUE, oobag.curve=TRUE, overlay=TRUE, method="OOB")
png("CVerror_by tree_withBEST.png",
        width = 6,
        height = 4,
        units = 'in',
        bg = "transparent",
        res = 300)
gbm.perf(boost.MSLT, plot.it=TRUE, oobag.curve=FALSE, overlay=TRUE, method="cv")
dev.off()
OOB.best.iter <- gbm.perf(boost.MSLT,method="OOB")
print(OOB.best.iter)
CV.best.iter <- gbm.perf(boost.MSLT,method="cv")
print(CV.best.iter)
summary(boost.MSLT,n.trees=1) 
summary(boost.MSLT,n.trees=OOB.best.iter)
summary(boost.MSLT,n.trees=CV.best.iter)
summary(boost.MSLT,n.trees=boost.MSLT$n.trees)

#****Printing of first and last trees
print(pretty.gbm.tree(boost.MSLT,1))
plot(pretty.gbm.tree(boost.MSLT,1))
print(pretty.gbm.tree(boost.MSLT,OOB.best.iter))
plot(pretty.gbm.tree(boost.MSLT,OOB.best.iter))
print(pretty.gbm.tree(boost.MSLT,CV.best.iter))
plot(pretty.gbm.tree(boost.MSLT,CV.best.iter))
print(pretty.gbm.tree(boost.MSLT,boost.MSLT$n.trees))
plot(pretty.gbm.tree(boost.MSLT,boost.MSLT$n.trees))

png("Boost_variable_importance_CVbestiter.png",
    width = 6,
    height = 6,
    units = 'in',
    bg = "transparent",
    res = 600)
summary(boost.MSLT,n.trees=CV.best.iter)[1]
dev.off()
write.csv(summary(boost.MSLT,
                  n.trees=CV.best.iter,
                  plotit=FALSE),
          "Boost_variable_importance_CVbestiter.csv")

png("CVerror_byTrees.png",
    width = 4,
    height = 4,
    units = 'in',
    bg = "transparent",
    res = 300)
plot(boost.MSLT$cv.error)
dev.off()
```

```{r partial plots for best 50% of variables from CV-best iteration}
#explore variable partial plots for variables in to 50% of relativel influence
exploreCVvars <- summary(boost.MSLT,CV.best.iter,plotit=F)[which(summary(boost.MSLT,CV.best.iter,plotit=F)$rel.inf>=median(summary(boost.MSLT,CV.best.iter,plotit=F)$rel.inf)),]
rel.inf.CVvars <- row.names(exploreCVvars)

#legends based on: https://stackoverflow.com/questions/20328452/legend-for-random-forest-plot-in-r
for(i in rel.inf.CVvars) {
    png(paste(i,"CV-best iteration gbm partial plot.png"),
        width = 4,
        height = 4,
        units = 'in',
        bg = "transparent",
        res = 300)
    plot(boost.MSLT,CV.best.iter,i=i)
    legend("right",
           boost.MSLT$classes,
           col=1:boost.MSLT$num.classes,
           cex=0.8,
           fill=1:boost.MSLT$num.classes)
    dev.off()
}
```

```{r get model performance in the best iteration, by CV}
boostData = MSLTcc
#library(caret)
summary(boostData)
set.seed(123)
#Model training without parameter definition
MSLTmodel2 = train(diagnosis2 ~ .,
                   data=boostData,
                   method="gbm",
                   distribution="multinomial")
MSLTmodel2
confusionMatrix(MSLTmodel2)

#Controls parameters/computational nuances for Train Function; requiring the training function to be based off of cross validation
fitControl = trainControl(method="cv", number=5, returnResamp = "all")
#Training to select best possible model based off of performance measures; ultimately select the best model
MSLTmodel = train(diagnosis2 ~ .,
                  data=boostTrain,
                  method="gbm",
                  distribution="multinomial",
                  trControl=fitControl)
MSLTmodel

MSLTmodel.predict <- predict(MSLTmodel, boostTest)

gbmCM <- confusionMatrix(MSLTmodel.predict, boostTest$diagnosis2)
gbmCM$table
paste("gbm accuracy in TEST data:",round(gbmCM$overall[[1]],2))
paste("gbm tree p-value:",gbmCM$overall[[6]])
gbmCM$byClass 

testacc <- as.data.frame(read.csv("TESTaccuracies.csv"))
fortestacc <- data.frame("Model"="gbmBestTreeByCV",
                         "TESTaccuracy"=gbmCM$overall[[1]],
                         "Date"=as.character(Sys.Date()),
                         "NCaccuracy"=gbmCM$byClass[1,"Balanced Accuracy"],
                         "IHaccuracy"=gbmCM$byClass[2,"Balanced Accuracy"],
                         "NT1accuracy"=gbmCM$byClass[3,"Balanced Accuracy"],
                         "NT2accuracy"=gbmCM$byClass[4,"Balanced Accuracy"],
                         stringsAsFactors = F)
testacc <- rbind(testacc,fortestacc)
write.csv(testacc,"TESTaccuracies.csv",row.names = F)

fitdf <- data.frame("obs"=boostTest$diagnosis2,"pred"=MSLTmodel.predict)
cats <- c(levels(boostData$diagnosis2)[1],
          levels(boostData$diagnosis2)[2],
          levels(boostData$diagnosis2)[3],
          levels(boostData$diagnosis2)[4])
mnLogLoss(fitdf, lev=cats) ###can't get this working becaues the fitdf dataframe doesn't have the needed columns (see ?mnLogLoss help file)
```

```{r k-fold_cross-validation, echo=FALSE}
k.folds <- function(k) {
    k=5
    folds <- createFolds(MSLTcc$diagnosis2, k = k, list = TRUE, returnTrain = TRUE)
    for (i in 1:k) {
        ###make model "MSLTmodel"
        MSLTmodel = train (diagnosis2 ~ .,
                           data=boostTrain,
                           method="gbm",
                           distribution="multinomial",
                           trControl=fitControl)

        boost.predictions = predict(MSLTmodel, boostTrain[-folds[[i]],], na.action = na.pass)
    
        accuracies.dt <- c(accuracies.dt, 
                           confusionMatrix(boost.predictions,
                                           boostTrain[-folds[[i]],]$diagnosis2)$overall[[1]])
    }
    accuracies.dt
}
```

```{r run the 5-fold cross validation}
set.seed(567)
accuracies.dt <- c()
accuracies.dt <- k.folds(5) 
accuracies.dt
mean.accuracies <- mean(accuracies.dt)
mean.accuracies
```

#####################################
#Repeated (200x) K-Fold Cross-Validation
```{r 200 iterations of 5-fold CV}
#set.seed(567)
#v <- c()
#v <- replicate(200, k.folds(5))
#accuracies.dt <- c()

#for (i in 1 : 200) { 
#  accuracies.boost.dt <- c(accuracies.dt, v[,i])
#}
accuracies.dt <- c()
set.seed(567)
v <- c()
v <- replicate(25, k.folds(5))
accuracies.567 <- c()
for (i in 1 : 25) { 
  accuracies.567 <- c(accuracies.567, v[,i])
}
save.image(file="reboot.567.RData")
accuracies.dt <- c()
set.seed(568)
v <- c()
v <- replicate(25, k.folds(5))
accuracies.568 <- accuracies.567
for (i in 1 : 25) { 
  accuracies.568 <- c(accuracies.568, v[,i])
}
save.image(file="reboot.568.RData")
accuracies.dt <- c()
set.seed(569)
v <- c()
v <- replicate(25, k.folds(5))
accuracies.569 <- accuracies.568
for (i in 1 : 25) { 
  accuracies.569 <- c(accuracies.569, v[,i])
}
save.image(file="reboot.569.RData")
accuracies.dt <- c()
set.seed(570)
v <- c()
v <- replicate(25, k.folds(5))
accuracies.570 <- accuracies.569
for (i in 1 : 25) { 
  accuracies.570 <- c(accuracies.570, v[,i])
}
save.image(file="reboot.570.RData")
accuracies.dt <- c()
set.seed(571)
v <- c()
v <- replicate(25, k.folds(5))
accuracies.571 <- accuracies.570
for (i in 1 : 25) { 
  accuracies.571 <- c(accuracies.571, v[,i])
}
save.image(file="reboot.571.RData")
accuracies.dt <- c()
set.seed(572)
v <- c()
v <- replicate(25, k.folds(5))
accuracies.572 <- accuracies.571
for (i in 1 : 25) { 
  accuracies.572 <- c(accuracies.572, v[,i])
}
save.image(file="reboot.572.RData")
accuracies.dt <- c()
set.seed(573)
v <- c()
v <- replicate(25, k.folds(5))
accuracies.573 <- accuracies.572
for (i in 1 : 25) { 
  accuracies.573 <- c(accuracies.573, v[,i])
}
save.image(file="reboot.573.RData")
accuracies.dt <- c()
set.seed(574)
v <- c()
v <- replicate(25, k.folds(5))
accuracies.574 <- accuracies.573
for (i in 1 : 25) { 
  accuracies.574 <- c(accuracies.574, v[,i])
}
save.image(file="boost200iter.RData")
accuracies.dt <- accuracies.574

mean.boost.accuracies <- mean(accuracies.dt)
mean.boost.accuracies
lci.boost <- mean(accuracies.dt) - sd(accuracies.dt) * 1.96
uci.boost <- mean(accuracies.dt) + sd(accuracies.dt) * 1.96
lci.boost
uci.boost

write.csv(accuracies.dt, "boost200.csv")
```

```{r Confidence Score Generation}
predict.gbm.confscore <- predict(object = boost.MSLT,
                                 newdata = boostTest,
                                 type = "response") #Uses the best iteration from the CV model - i.e. MSLTmodel
predict.gbm.confscore #these are votes for each category from the gbm "forest", per individual
boost.MSLT.test = gbm(diagnosis2~.,
                      data=boostTest,
                      distribution="multinomial",
                      n.trees=10000, shrinkage=0.01,
                      interaction.depth=4,
                      cv.folds=5,
                      bag.fraction=0.5) 
predict.gbm.confscore2 <- predict(object = boost.MSLT.test, newdata=boostTrain, type="response")
predict.gbm.confscore2
setwd("C:/Users/xj901087/Dropbox")
write.csv(predict.gbm.confscore, file = "boosting_total_predictions1.csv")
write.csv(predict.gbm.confscore2, file = "boosting_total_predictions2.csv")

#In fact, I think the appropriate way to do this is to use the chosen model to predict all individuals (train and test), not to generate a separate model and backward calculate
GBMpred <- predict(boost.MSLT,
                   MSLTcc,
                   type="response")
str(GBMpred)
GBMconf <- as.data.frame(GBMpred)
for(i in 1:nrow(GBMconf)) {
    #need to make sure it only assesses category columns, hence 1:4
    pmax <- max(GBMconf[i,1:4])
    psecondmax <- sort(GBMconf[i,1:4],
                       partial=length(GBMconf[i,1:4])-1)[[length(GBMconf[i,1:4])-1]]
    GBMconf[i,"confidence_score"] <- ifelse(psecondmax==0,10,min((pmax/psecondmax - 1),10))
}
names(GBMconf) <- c("NC","IH","NT1","NT2","confidence_score")

#check confidence score by category
NCsub <- GBMconf[which(max.col(GBMconf[,1:4])==1),]
paste("NC confidence scores:",
      round(mean(NCsub$confidence_score),2),
      "+/-",
      round(sd(NCsub$confidence_score),2))
IHsub <- GBMconf[which(max.col(GBMconf[,1:4])==2),]
paste("IH confidence scores:",
      round(mean(IHsub$confidence_score),2),
      "+/-",
      round(sd(IHsub$confidence_score),2))
NT1sub <- GBMconf[which(max.col(GBMconf[,1:4])==3),]
paste("NT1 confidence scores:",
      round(mean(NT1sub$confidence_score),2),
      "+/-",
      round(sd(NT1sub$confidence_score),2))
NT2sub <- GBMconf[which(max.col(GBMconf[,1:4])==4),]
paste("NT2 confidence scores:",
      round(mean(NT2sub$confidence_score),2),
      "+/-",
      round(sd(NT2sub$confidence_score),2))
```

```{r ANOVA of confidence scores by category}
#Check if these confidence scores are significantly different between groups
talldata <- data.frame("category"=character(),"confidence_score"=numeric())
NC4TD <- data.frame("category"=rep("NC",length.out=nrow(NCsub)),
                    "confidence_score"=as.numeric(NCsub[,"confidence_score"]))
IH4TD <- data.frame("category"=rep("IH",length.out=nrow(IHsub)),
                    "confidence_score"=as.numeric(IHsub[,"confidence_score"]))
NT14TD <- data.frame("category"=rep("NT1",length.out=nrow(NT1sub)),
                     "confidence_score"=as.numeric(NT1sub[,"confidence_score"]))
NT24TD <- data.frame("category"=rep("NT2",length.out=nrow(NT2sub)),
                     "confidence_score"=as.numeric(NT2sub[,"confidence_score"]))
talldata <- rbind(talldata,
                  NC4TD,
                  IH4TD,
                  NT14TD,
                  NT24TD)
attach(talldata)
Anova(lm(confidence_score ~ category,
         data=talldata),
      type="III")
cat2conf <- aov(lm(confidence_score ~ category))
cat2conf
posthoc <- TukeyHSD(x=cat2conf, 'category', conf.level = 0.95)
posthoc
hsdcat2conf <- HSD.test(cat2conf, "category")
hsdcat2conf
#They are
```


```{r prep data for ROC curve}
setwd("C:/Users/xj901087/Dropbox/")
MSLT_ROC <- read.csv("MASTER4R_ROCcurve.csv")
summary(MSLT_ROC)
head(MSLT_ROC)
levels(MSLT_ROC$diagnosis)
MSLT_ROC$nom <- factor(MSLT_ROC$diagnosis)
levels(MSLT_ROC$nom)
MSLT_ROC$diagnosis2 <- relevel(MSLT_ROC$nom, ref = "4")
MSLTsub_ROC <- MSLT_ROC[,-6]
nc<-MSLTsub_ROC[which(MSLTsub_ROC$diagnosis==4),]
head(nc)
tail(nc)
summary(nc)
str(nc)
ncomplete <- nc[complete.cases(nc),]
head(ncomplete)
tail(ncomplete)
summary(ncomplete)
str(ncomplete)
MSLTnew_ROC<-rbind.data.frame(MSLTsub_ROC[which(MSLTsub_ROC$diagnosis != 4),],ncomplete)
summary(MSLTnew_ROC)
MSLTglmnet_ROC <- MSLTnew_ROC[,c(25,2,3,4,5,6,7,8,9,10,11,12,13,14)] 
MSLTcc_ROC <- MSLTglmnet_ROC[complete.cases(MSLTglmnet_ROC),]
summary(MSLTcc_ROC)

#subsetting traing/test sets
set.seed(123)
ROCpart <- createDataPartition(MSLTcc_ROC$diagnosis2,
                                 times=1,
                                 p=0.625,
                                 list=F)
boostTrain_ROC <- MSLTcc_ROC[ROCpart,]
summary(boostTrain_ROC)
boostTest_ROC <- MSLTcc_ROC[-ROCpart,]
summary(boostTest_ROC)
###9/30 as a note to fix, the summary of MSLTcc and MSLTcc_ROC highlights that we're dealing with different data sets (based on sample size differences)!

#sent environmental training parameters
fitControl = trainControl(method="cv", number=5, returnResamp = "all")
```

```{r category (NT1, NT2, IH, NC) vs others}
catlist <- names(MSLTcc_ROC)[2:5]
MSLTccsub <- MSLTcc_ROC[,c(6:ncol(MSLTcc_ROC))]
AUCout <- data.frame("ROC_category"=character(),
                     "AUC"=numeric(),
                     stringsAsFactors = F)
k=1
for (i in catlist) {
    MSLTcc_ROC_cat <- cbind(MSLTcc_ROC[,i],MSLTccsub)
    names(MSLTcc_ROC_cat)[1] <- i
    summary(MSLTcc_ROC_cat)
    MSLTcc_ROC_cat$nom <- as.factor(MSLTcc_ROC_cat[,i])
    levels(MSLTcc_ROC_cat$nom)
    summary(MSLTcc_ROC_cat)
    MSLTcc_ROC_cat[,i] <- relevel(MSLTcc_ROC_cat$nom, ref = "0")
    MSLTcc_ROC_cat <- MSLTcc_ROC_cat[,-ncol(MSLTcc_ROC_cat)]
    summary(MSLTcc_ROC_cat)
    
    #training/test partitioning
    set.seed(123)
    train_ROC_cat <- createDataPartition(MSLTcc_ROC_cat[,i],
                                         times=1,
                                         p=0.625,
                                         list=F)
    boostTrain_ROC_cat = MSLTcc_ROC_cat[train_ROC_cat,]
    summary(boostTrain_ROC_cat)
    boostTest_ROC_cat=MSLTcc_ROC_cat[-train_ROC_cat,]
    summary(boostTest_ROC_cat)
    
    #cat_formula <- noquote(paste(i,"~ ."))...couldn't use because train wouldn't accept formula
    trainprep <- boostTrain_ROC_cat
    trainprep$Classes <- as.factor(ifelse(boostTrain_ROC_cat[,i]==1,i,"other"))
    trainprep$Classes <- relevel(trainprep$Classes,ref="other")
    xTrain <- trainprep[,c(2:(ncol(trainprep)-1))]
    yTrain <- trainprep[,1]
    
    boost.train.ROC.cat <- train(x=xTrain,
                                 y=yTrain,
                                 method = "gbm",
                                 trControl=fitControl)
    boost.train.ROC.cat
    boost.predict.ROC.cat <- predict(boost.train.ROC.cat, boostTest_ROC_cat)
    str(boost.predict.ROC.cat)
    confusionMatrix(boost.predict.ROC.cat, boostTest_ROC_cat[,i])
    boost.probs.ROC.cat <- predict(boost.train.ROC.cat,
                                   boostTest_ROC_cat,
                                   type="prob")
    #library(pROC)
    boost.rocCurve.cat <- roc(response = boostTest_ROC_cat[,i],
                              predictor = boost.probs.ROC.cat[, "1"],
                              levels = rev(levels(boostTest_ROC_cat[,i])))
    png(paste("boostROC_",i,"vOther_bw%02d.png",sep=""),
        width = 4,
        height = 4,
        units = 'in',
        bg = "transparent",
        res = 300)
    plot(boost.rocCurve.cat)
    dev.off()
    paste("gbm ROC curve AUC for",i,"vs others:",round(boost.rocCurve.cat$auc[1],2))
    
    AUCout[k,"ROC_category"] <- i
    AUCout[k,"AUC"] <- boost.rocCurve.cat$auc[1]
    k=k+1
}
write.csv(AUCout,"boostAUCs.csv")
```